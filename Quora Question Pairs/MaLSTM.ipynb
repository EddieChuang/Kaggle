{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_file = 'data/train.csv'\n",
    "test_file = 'data/test.csv'\n",
    "glove_file = 'wordvectors/glove.6B.300d.txt'\n",
    "unk_char = '<UNK>'\n",
    "pad_char = '<PAD>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(train_file, word_to_index, max_len=None):\n",
    "    sentence_pair, is_duplicated, sequence_length = read_train_data(train_file)\n",
    "    sent_pair = filter_unknown(sentence_pair, word_to_index, unk_char)\n",
    "    if max_len:\n",
    "        sent_pair = pad(sent_pair, pad_char, max_len)\n",
    "    \n",
    "    return word2index(sent_pair, word_to_index), np.array(is_duplicated), np.array(sequence_length)\n",
    "\n",
    "def get_test_data(test_file, word_to_index):\n",
    "    sentence_pair, sequence_length = read_test_data(test_file)\n",
    "    sent_pair = filter_unknown(sentence_pair, word_to_index, unk_char)\n",
    "    \n",
    "    return word2index(sent_pair, word_to_index), np.array(sequence_length)\n",
    "\n",
    "\n",
    "def get_embedding_matrix(glove_file, unk_char, pad_char):\n",
    "    word_to_index, index_to_word, emb_matrix = read_glove_vecs(glove_file)\n",
    "\n",
    "    word_to_index[unk_char] = len(word_to_index)\n",
    "    word_to_index[pad_char] = len(word_to_index)\n",
    "\n",
    "    index_to_word[len(word_to_index) - 2] = unk_char\n",
    "    index_to_word[len(word_to_index) - 1] = pad_char\n",
    "\n",
    "    emb_dim = emb_matrix.shape[1]\n",
    "    emb_matrix = np.append(emb_matrix, [[0] * emb_dim] * 2, axis=0)\n",
    "\n",
    "    return emb_matrix, word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix, word_to_index, index_to_word = get_embedding_matrix(glove_file, unk_char, pad_char)\n",
    "print('emb_matrix shape: {}'.format(emb_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_len = 20\n",
    "# x_train, y_train, train_sequlence_length = get_train_data(train_file, word_to_index, max_len=max_len)\n",
    "# np.save('data/x_train_pad.npy', x_train)\n",
    "# np.save('data/y_train.npy', y_train)\n",
    "# np.save('data/train_seq_len.npy', train_sequlence_length)\n",
    "# print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test, test_sequence_length = get_test_data(test_file, word_to_index)\n",
    "# np.save('data/test.npy', x_test)\n",
    "# np.save('data/test_seq_len.npy', test_sequence_length)\n",
    "# print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.backend import binary_crossentropy\n",
    "from tensorflow.contrib.layers import xavier_initializer\n",
    "from tensorflow.nn import bidirectional_dynamic_rnn, embedding_lookup, dropout\n",
    "from tensorflow.contrib.rnn import LSTMCell, MultiRNNCell\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "class QuestionPairDuplicated:\n",
    "    def __init__(self, emb_matrix, learning_rate, emb_trainable=False):\n",
    "        tf.reset_default_graph()\n",
    "        self.learning_rate = learning_rate\n",
    "                \n",
    "        # input\n",
    "        self.input_sentA = tf.placeholder(tf.int32, shape=[None, None])  # (batch_size, time_step)\n",
    "        self.input_sentB = tf.placeholder(tf.int32, shape=[None, None])  # (batch_size, time_step) \n",
    "        self.input_seq_lenA = tf.placeholder(tf.int32, shape=[None, ])  # (batch_size, )\n",
    "        self.input_seq_lenB = tf.placeholder(tf.int32, shape=[None, ])  # (batch_size, )\n",
    "        \n",
    "        # output\n",
    "        self.target = tf.placeholder(dtype=tf.float32, shape=[None, ])  # (batch_size, )\n",
    "\n",
    "        # embedding matrix\n",
    "        self.embedding_matrix = tf.get_variable(shape=emb_matrix.shape, \n",
    "                                    initializer=tf.constant_initializer(emb_matrix, dtype=tf.float32),\n",
    "                                    dtype=tf.float32,\n",
    "                                    trainable=emb_trainable,\n",
    "                                    name='embeddings_matrix')\n",
    "        \n",
    "        \n",
    "    def embedding_layer(self, sequence):\n",
    "        return embedding_lookup(self.embedding_matrix, sequence)  # (batch_size, time_step, emb_dim)\n",
    "    \n",
    "    \n",
    "    def bilstm(self, sequence, sequence_length, lstm_unit, reuse=None):\n",
    "        with tf.variable_scope('BiLSTM', reuse=reuse, dtype=tf.float32):\n",
    "            cell_fw = LSTMCell(num_units=lstm_unit, reuse=tf.get_variable_scope().reuse)\n",
    "            cell_bw = LSTMCell(num_units=lstm_unit, reuse=tf.get_variable_scope().reuse)\n",
    "            \n",
    "        ((output_fw, output_bw), _) = bidirectional_dynamic_rnn(cell_fw, cell_bw, sequence, dtype=tf.float32, sequence_length=sequence_length)\n",
    "        \n",
    "        return tf.concat([output_fw, output_bw], axis=2)  # (batch_size, num_step, lstm_unit * 2)\n",
    "    \n",
    "    \n",
    "    def lstm(self, sequence, sequence_length, lstm_unit, n_layers=1, reuse=None):\n",
    "        with tf.variable_scope('LSTM', reuse=reuse, dtype=tf.float32):\n",
    "            cell = tf.contrib.rnn.LSTMCell(num_units=lstm_unit, activation='tanh', reuse=tf.get_variable_scope().reuse)\n",
    "#             cell = MultiRNNCell([cell] * n_layers)\n",
    "\n",
    "        _, state = tf.nn.dynamic_rnn(cell, sequence, dtype=tf.float32, sequence_length=sequence_length)\n",
    "        return state[1]  # (batch_size, lstm_unit)\n",
    "    \n",
    "    \n",
    "    def manhattan_distance(self, vecA, vecB):\n",
    "        # exp(-||h1 - h2||)\n",
    "        diff = tf.reduce_sum(tf.abs(tf.subtract(vecA, vecB)), axis=1)  # (batch_size, )\n",
    "        return tf.exp(-diff)\n",
    "    \n",
    "    \n",
    "    def loss_function(self, output):\n",
    "        ## MSE\n",
    "        diff = tf.subtract(self.target, output) # (batch_size, )\n",
    "        return tf.reduce_mean(tf.square(diff)) # (1, )\n",
    "    \n",
    "    \n",
    "    def build(self, lstm_unit=256, hidden_unit=16, output_unit=1, encoder='lstm'):\n",
    "        word_embA = self.embedding_layer(self.input_sentA)  # (batch_size, num_step, emb_dim)\n",
    "        word_embB = self.embedding_layer(self.input_sentB)  # (batch_size, num_step, emb_dim)\n",
    "        \n",
    "        if encoder == 'lstm':\n",
    "            repA = self.lstm(word_embA, self.input_seq_lenA, lstm_unit, reuse=None)  # (batch_size, lstm_unit)\n",
    "            repB = self.lstm(word_embB, self.input_seq_lenB, lstm_unit, reuse=True)  # (batch_size, lstm_unit)\n",
    "            input_dim = lstm_unit * 2\n",
    "        elif encoder == 'bilstm':\n",
    "            repA = self.bilstm(word_embA, self.input_seq_lenA, lstm_unit, None)  # (batch_size, num_step, lstm_unit * 2)\n",
    "            repB = self.bilstm(word_embA, self.input_seq_lenB, lstm_unit, True)  # (batch_size, num_step, lstm_unit * 2)\n",
    "            repA = tf.reduce_sum(repA, axis=1)  # (batch_size, lstm_unit * 2)\n",
    "            repB = tf.reduce_sum(repB, axis=1)  # (batch_size, lstm_unit * 2)\n",
    "            input_dim = lstm_unit * 4\n",
    "        \n",
    "        self.output = self.manhattan_distance(repA, repB)  # (batch_size, )\n",
    "        self.loss = self.loss_function(self.output)  # (1, )\n",
    "        \n",
    "#         rep = tf.concat([repA, repB], axis=1)  # lstm: (batch_size, lstm_unit * 2), bilstm: (batch_size, lstm_unit * 4)\n",
    "#         rep = dropout(rep, keep_prob=0.8)\n",
    "        \n",
    "#         hidden = self.dense(rep, hidden_unit, 'hidden')  # (batch_size, hidden_unit)\n",
    "#         hidden = dropout(hidden, keep_prob=0.8)\n",
    "        \n",
    "#         self.output = self.dense(hidden, output_unit, 'output')  # (batch_size, output_unit)\n",
    "#         self.output = tf.reshape(self.output, (-1, ))\n",
    "\n",
    "#         self.loss = self.loss_function(self.output)  # ()\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        \n",
    "    def fit(self, train_data, val_data=None, epoch_size=1, batch_size=128, word_to_index=None, model_name='model'):\n",
    "        def learn(X, Y, sequence_length, epoch, mode):\n",
    "            tn = tqdm_notebook(total=len(X))\n",
    "            for sentA, sentB, seq_lenA, seq_lenB, target in next_batch_with_pad(X, Y, sequence_length, word_to_index, batch_size):\n",
    "#             for sentA, sentB, seq_lenA, seq_lenB, target in next_batch(X, Y, sequence_length, batch_size):\n",
    "\n",
    "                feed_dict = {\n",
    "                    self.input_sentA: sentA,\n",
    "                    self.input_sentB: sentB, \n",
    "                    self.input_seq_lenA: seq_lenA,\n",
    "                    self.input_seq_lenB: seq_lenB,\n",
    "                    self.target: target\n",
    "                }\n",
    "                if mode == 'train':\n",
    "                    fetches = [self.loss, self.output, self.optimizer]\n",
    "                    loss, output, _ = self.sess.run(fetches, feed_dict)\n",
    "                    tn.set_description('Epoch: {}/{}'.format(epoch, epoch_size))\n",
    "                elif mode == 'validate':\n",
    "                    fetches = [self.loss, self.output]\n",
    "                    loss, output = self.sess.run(fetches, feed_dict)\n",
    "                \n",
    "                tn.set_postfix(loss=loss, accuracy=accuracy(output, target), mode=mode)\n",
    "                tn.update(n=batch_size)\n",
    "                \n",
    "        saver = tf.train.Saver()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "                \n",
    "        x_train, y_train, train_sequence_length = train_data[0], train_data[1], train_data[2]\n",
    "        if val_data:\n",
    "            x_val, y_val, val_sequence_length = val_data[0], val_data[1], val_data[2]\n",
    "        \n",
    "        print('Train on {} samples, validate on {} samples'.format(len(x_train), len(x_val) if val_data else 0))\n",
    "        for epoch in range(1, epoch_size + 1):       \n",
    "            x_train, y_train, train_sequence_length = shuffle_data(x_train, y_train, train_sequence_length)\n",
    "            # train\n",
    "            learn(x_train, y_train, train_sequence_length, epoch, 'train')\n",
    "\n",
    "            # validate\n",
    "            if val_data:\n",
    "                learn(x_val, y_val, val_sequence_length, epoch, 'validate')    \n",
    "\n",
    "\n",
    "        save_path = saver.save(self.sess, 'models/{}.ckpt'.format(model_name))\n",
    "        print('Model was saved in {}'.format(save_path))\n",
    "            \n",
    "    \n",
    "    def restore(self, model_path):\n",
    "        saver = tf.train.Saver()\n",
    "        self.sess = tf.Session()\n",
    "        saver.restore(self.sess, model_path)\n",
    "            \n",
    "    \n",
    "    def predict(self, X, sequence_length, word_to_index):\n",
    "        \n",
    "        y_empty = np.empty(0)\n",
    "        batch_size, i = 100, 0\n",
    "        tn = tqdm_notebook(total=len(X))\n",
    "        prediction = np.empty((len(X), ))\n",
    "        for sentA, sentB, seq_lenA, seq_lenB, _ in next_batch_with_pad(X, y_empty, sequence_length, word_to_index, batch_size):\n",
    "            fetches = [self.output]\n",
    "            feed_dict = {\n",
    "                self.input_sentA: sentA,\n",
    "                self.input_sentB: sentB, \n",
    "                self.input_seq_lenA: seq_lenA,\n",
    "                self.input_seq_lenB: seq_lenB,\n",
    "            }\n",
    "            output = self.sess.run(fetches, feed_dict)[0]\n",
    "            prediction[i * batch_size: i * batch_size + len(output)] = output\n",
    "            \n",
    "            tn.set_postfix(mode='predict')\n",
    "            tn.update(n=batch_size)\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        \n",
    "        return prediction\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, sequence_length = np.load('data/x_train.npy'), np.load('data/y_train.npy'), np.load('data/train_seq_len.npy')\n",
    "x_train, y_train, train_sequlence_length, x_val, y_val, val_sequlence_length = split_train_val_data(X, Y, sequence_length, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "model = QuestionPairDuplicated(emb_matrix, learning_rate, emb_trainable=True)\n",
    "model.build(lstm_unit=256, encoder='lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch_size = 15\n",
    "batch_size = 128\n",
    "train_data = [x_train, y_train, train_sequlence_length]\n",
    "val_data = [x_val, y_val, val_sequlence_length]\n",
    "model.fit(train_data, None, epoch_size, batch_size, word_to_index, 'model-manhattan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/model-manhattan.ckpt'\n",
    "model.restore(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, test_sequence_length = np.load('data/x_test.npy'), np.load('data/test_seq_len.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(x_test, test_sequence_length, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction2csv(prediction, 'data/submit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
